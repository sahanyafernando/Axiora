{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMDcu/rYSpBrPDjG1O0gwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/Axiora/blob/main/Subword_Tokenization_in_Real_World_Scenarios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subword Tokenization in Real-World Scenarios"
      ],
      "metadata": {
        "id": "OH7JgSVsYyeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Objective***: Understand how different subword tokenizers work and why they matter in real-world NLP tasks\n",
        "\n",
        "***Scenario***: Think that when we are building a multilingual chatbot. Some users type rare or newly coined words like \"ecoengineer\", \"hyperautomation\".\n",
        "\n",
        "***Question***: How do we ensure our model can handdle such words it hasn't seen during training ?"
      ],
      "metadata": {
        "id": "ee8WYE-RZFKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1: Rare words confuse the model"
      ],
      "metadata": {
        "id": "hU55utSKaJnE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eoNgdeUXWsY"
      },
      "outputs": [],
      "source": [
        "# Let's simulate a sentence from a user\n",
        "sentence = \"Our ecoengineer developed a hyperautomation pipeline\"\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenizing with wordPiece (used in BERT)\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"wordPiece Tokens:\", tokens)\n",
        "\n",
        "# This gives the output below:\n",
        "# wordPiece Tokens: ['our', 'eco', '##eng', '##ine', '##er', 'developed', 'a', 'hyper', '##au', '##tom', '##ation', 'pipeline']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2: Laguages Without Spaces (like Chinese, Japanese)"
      ],
      "metadata": {
        "id": "6yhRmllWcG0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine a user types in Japanese or a compound Hindi word with no clear word boundaries"
      ],
      "metadata": {
        "id": "McUdwHV8dIkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train model on toy data\n",
        "with open(\"data.txt\", \"w\") as f:\n",
        "    f.write(\"Our ecoengineer hyperautomation AI automation intelligence\\n\")\n",
        "\n",
        "spm.SentencePieceTrainer.Train(input='data.txt', model_prefix='bpe_demo', vocab_size=40, model_type='bpe')\n",
        "\n",
        "# Load and tokenize\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_demo.model')\n",
        "print(sp.encode('ecoengineer hyperautomation', out_type=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB3IH9zadHn1",
        "outputId": "3bfc4c37-196e-404c-ba0e-0a4915956cbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'e', 'co', 'en', 'g', 'in', 'e', 'er', '▁', 'hy', 'p', 'er', 'automation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentencePiece can handle raw text without whitespase splitting. Very usefull for multiingual models or when users enter creative spellings."
      ],
      "metadata": {
        "id": "zJJdai1CrI2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 3: Training a Tokenizer for Your Domain"
      ],
      "metadata": {
        "id": "dkF1GKllrvE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think you are working in the healthcare domain and users type complex medical terms. You wanna train a custom tokenizer that understands medical jargon.\n",
        "\n",
        "Let's train a tiny BPE tokenizer on a small domain-specific corpus."
      ],
      "metadata": {
        "id": "jRQaigivsVqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# Simulated medical terms\n",
        "texts = [\n",
        "    \"neuroplasticity\", \"neurogenisis\", \"immunotherapy\",\n",
        "    \"cardiomyopathy\", \"pharmacogenomics\", \"biocompatibility\"\n",
        "]\n",
        "\n",
        "tokenizer_bpe = Tokenizer(models.BPE())\n",
        "tokenizer_bpe.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "trainer = trainers.BpeTrainer(vocab_size=40, show_progress=False)\n",
        "\n",
        "tokenizer_bpe.train_from_iterator(texts, trainer)\n",
        "\n",
        "output = tokenizer_bpe.encode(\"biocompatibility and neuroplasticity\")\n",
        "print(\"BPE Tokens\", output.tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP5KAtmRrvh5",
        "outputId": "1e149e7d-1fcc-4b4b-c90a-e99307aa1d92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE Tokens ['bi', 'o', 'com', 'p', 'at', 'i', 'bi', 'l', 'ity', 'a', 'n', 'd', 'neur', 'op', 'l', 'as', 't', 'ic', 'ity']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE learns the most frequent subword units from your domain data, which is especially helpful when public tokenizers miss important terms."
      ],
      "metadata": {
        "id": "ew9Ttc9PvuSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About N-Grams ?"
      ],
      "metadata": {
        "id": "qB01hm7LN211"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n=2):\n",
        "  words = text.split()\n",
        "  return [words[i:i+n] for i in range(len(words)-n+1)]\n",
        "\n",
        "generate_ngrams(\"Language models learn patterns\", n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR7hcB2yN9R-",
        "outputId": "de7de5a9-589d-45ea-9c4c-1b672256fa47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Language', 'models', 'learn'], ['models', 'learn', 'patterns']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N_Grams are fixed-size word chunks often used in traditonal models but don't adapt to unkown word structure like subwords.\n",
        "\n",
        "### Now, What We Have Covered ?\n",
        "\n",
        "- Subword tokenization is essential for rare or new words\n",
        "- WordPiece (BERT): breaks words into knows base + suffixes\n",
        "- SentencePiece: great for multilingual/raw text handling\n",
        "- BPE: builds vocab from most frequent character merges, great for domain adaption.\n",
        "- N-Grams: Traditonal but less flexible for rare/new terms.\n"
      ],
      "metadata": {
        "id": "SB3r0ffVO7L-"
      }
    }
  ]
}